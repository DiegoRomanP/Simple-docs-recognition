"""
Sistema RAG local para PDFs con configuraci√≥n flexible
"""

import os
from typing import List, Dict, Optional
from pathlib import Path

# LangChain (versi√≥n ligera)
from langchain_community.document_loaders import PyPDFLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.embeddings import HuggingFaceEmbeddings
from langchain.vectorstores import Chroma
from langchain.prompts import PromptTemplate
from langchain.chains import RetrievalQA

# Para LLM local
from langchain_community.llms import Ollama  # Opci√≥n 1
# from langchain_community.llms import LlamaCpp  # Opci√≥n 2


class PDFRAGSystem:
    """Sistema completo RAG para PDFs locales"""
    
    def __init__(
        self,
        model_name: str = "llama3:8b",  # Ollama model name
        embeddings_model: str = "sentence-transformers/all-MiniLM-L6-v2",
        persist_directory: str = "./chroma_db",
        chunk_size: int = 1000,
        chunk_overlap: int = 200
    ):
        """
        Inicializa el sistema RAG.
        
        Args:
            model_name: Nombre del modelo LLM (Ollama o ruta .gguf)
            embeddings_model: Modelo para embeddings
            persist_directory: Donde guardar la base vectorial
            chunk_size: Tama√±o de fragmentos de texto
            chunk_overlap: Solapamiento entre fragmentos
        """
        self.persist_directory = persist_directory
        self.chunk_size = chunk_size
        self.chunk_overlap = chunk_overlap
        
        # 1. Inicializar embeddings (local)
        print(f"Cargando embeddings: {embeddings_model}")
        self.embeddings = HuggingFaceEmbeddings(
            model_name=embeddings_model,
            model_kwargs={'device': 'cpu'},  # o 'cuda' si tienes GPU
            encode_kwargs={'normalize_embeddings': True}
        )
        
        # 2. Inicializar LLM local
        print(f"Inicializando LLM: {model_name}")
        self.llm = self._initialize_llm(model_name)
        
        # 3. Vector store (inicializar m√°s tarde con documentos)
        self.vector_store = None
        self.retriever = None
        self.qa_chain = None
    
    def _initialize_llm(self, model_name: str):
        """Inicializa el LLM local seg√∫n el tipo"""
        
        # Opci√≥n 1: Ollama (recomendado para facilidad)
        if ":" in model_name or model_name in ["llama3", "mistral", "codellama"]:
            return Ollama(
                model=model_name,
                temperature=0.1,  # Bajo para respuestas consistentes
                num_predict=512,  # M√°ximo tokens a generar
            )
        
        # Opci√≥n 2: Llama CPP (modelos .gguf)
        elif model_name.endswith(".gguf"):
            from langchain_community.llms import LlamaCpp
            return LlamaCpp(
                model_path=model_name,
                n_ctx=2048,  # Contexto m√°ximo
                temperature=0.1,
                verbose=False
            )
        
        else:
            raise ValueError(f"Modelo no soportado: {model_name}")
    
    def load_and_process_pdf(
        self,
        pdf_path: str,
        clear_existing: bool = False
    ) -> None:
        """
        Carga y procesa un PDF para el RAG.
        
        Args:
            pdf_path: Ruta al archivo PDF
            clear_existing: Si True, borra la base existente
        """
        print(f"Procesando PDF: {pdf_path}")
        
        # 1. Cargar PDF
        loader = PyPDFLoader(pdf_path)
        documents = loader.load()
        
        # 2. Dividir en chunks
        text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=self.chunk_size,
            chunk_overlap=self.chunk_overlap,
            length_function=len,
            separators=["\n\n", "\n", " ", ""]
        )
        chunks = text_splitter.split_documents(documents)
        
        print(f"Documento dividido en {len(chunks)} chunks")
        
        # 3. Crear o cargar vector store
        if clear_existing and os.path.exists(self.persist_directory):
            import shutil
            shutil.rmtree(self.persist_directory)
        
        if os.path.exists(self.persist_directory):
            # Cargar existente
            print("Cargando base vectorial existente...")
            self.vector_store = Chroma(
                persist_directory=self.persist_directory,
                embedding_function=self.embeddings
            )
            # A√±adir nuevos documentos
            self.vector_store.add_documents(chunks)
        else:
            # Crear nueva
            print("Creando nueva base vectorial...")
            self.vector_store = Chroma.from_documents(
                documents=chunks,
                embedding=self.embeddings,
                persist_directory=self.persist_directory
            )
        
        # 4. Configurar retriever
        self.retriever = self.vector_store.as_retriever(
            search_type="similarity",
            search_kwargs={"k": 4}  # Traer 4 chunks m√°s relevantes
        )
        
        # 5. Crear cadena QA
        self._create_qa_chain()
        
        print("‚úÖ PDF procesado y listo para consultas")
    
    def _create_qa_chain(self):
        """Crea la cadena de pregunta-respuesta"""
        
        # Prompt personalizado para contexto de PDF
        prompt_template = """
        Eres un asistente especializado en analizar documentos PDF.
        Usa el siguiente contexto del PDF para responder la pregunta.
        Si no sabes la respuesta, di que no lo sabes, no inventes.
        
        Contexto del PDF:
        {context}
        
        Pregunta: {question}
        
        Respuesta detallada:"""
        
        PROMPT = PromptTemplate(
            template=prompt_template,
            input_variables=["context", "question"]
        )
        
        # Crear cadena de QA
        self.qa_chain = RetrievalQA.from_chain_type(
            llm=self.llm,
            chain_type="stuff",
            retriever=self.retriever,
            chain_type_kwargs={"prompt": PROMPT},
            return_source_documents=True
        )
    
    def ask_question(self, question: str) -> Dict:
        """
        Haz una pregunta sobre el contenido del PDF.
        
        Args:
            question: Pregunta a realizar
            
        Returns:
            Dict con respuesta y fuentes
        """
        if not self.qa_chain:
            raise ValueError("Primero carga un PDF con load_and_process_pdf()")
        
        print(f"Pregunta: {question}")
        result = self.qa_chain({"query": question})
        
        return {
            "answer": result["result"],
            "sources": [
                {
                    "content": doc.page_content[:200] + "...",
                    "page": doc.metadata.get("page", "N/A"),
                    "source": doc.metadata.get("source", "N/A")
                }
                for doc in result["source_documents"]
            ]
        }
    
    def chat_session(self):
        """Inicia una sesi√≥n interactiva de chat"""
        print("\n" + "="*50)
        print("Chat con tu PDF. Escribe 'salir' para terminar.")
        print("="*50)
        
        while True:
            try:
                question = input("\nüìù Tu pregunta: ").strip()
                
                if question.lower() in ['salir', 'exit', 'quit']:
                    print("üëã ¬°Hasta luego!")
                    break
                
                if not question:
                    continue
                
                respuesta = self.ask_question(question)
                
                print("\nü§ñ Respuesta:")
                print(respuesta["answer"])
                
                if respuesta["sources"]:
                    print("\nüìö Fuentes utilizadas:")
                    for i, source in enumerate(respuesta["sources"], 1):
                        print(f"{i}. P√°gina {source['page']}: {source['content']}")
                        
            except KeyboardInterrupt:
                print("\n\nInterrumpido por usuario")
                break
            except Exception as e:
                print(f"‚ùå Error: {e}")


# Uso simplificado
class SimplePDFChat:
    """Versi√≥n simplificada para empezar r√°pido"""
    
    @staticmethod
    def quick_start(pdf_path: str, model: str = "mistral"):
        """
        Inicio r√°pido: procesa PDF y abre chat.
        
        Args:
            pdf_path: Ruta al PDF
            model: Modelo a usar (llama3, mistral, etc.)
        """
        # Verificar que Ollama est√° instalado y el modelo descargado
        try:
            import subprocess
            result = subprocess.run(["ollama", "list"], capture_output=True, text=True)
            if model not in result.stdout:
                print(f"‚ö†Ô∏è  Modelo {model} no encontrado en Ollama.")
                print(f"Ejecuta: ollama pull {model}")
                return
        except:
            print("‚ö†Ô∏è  Ollama no encontrado. Instala desde: https://ollama.com/")
            return
        
        # Crear y usar el sistema
        rag = PDFRAGSystem(model_name=model)
        rag.load_and_process_pdf(pdf_path)
        rag.chat_session()


# Ejemplo de uso
if __name__ == "__main__":
    # Opci√≥n 1: Uso simple
    # SimplePDFChat.quick_start("documento.pdf", model="mistral")
    
    # Opci√≥n 2: Uso controlado
    rag_system = PDFRAGSystem(
        model_name="llama3:8b",
        embeddings_model="sentence-transformers/all-MiniLM-L6-v2",
        chunk_size=800,
        chunk_overlap=150
    )
    
    # Procesar m√∫ltiples PDFs
    rag_system.load_and_process_pdf("/home/diego/Documentos/proyecto_pam/pdfs/lecturas-para-todos-los-dias.pdf")
    # rag_system.load_and_process_pdf("contrato.pdf", clear_existing=False)  # Para a√±adir
    
    # Hacer preguntas
    resultado = rag_system.ask_question(
        "¬øCu√°les son los puntos principales del documento?"
    )
    
    print("Respuesta:", resultado["answer"])
    
    # O iniciar chat interactivo
    # rag_system.chat_session()import os
