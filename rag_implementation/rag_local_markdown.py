"""
Sistema RAG HIPER-OPTIMIZADO para Markdown
"""

import re
from pathlib import Path
from typing import List, Dict
import time

# Dependencias MUCHO m√°s ligeras que para PDF
import markdown
from bs4 import BeautifulSoup
import numpy as np
from sentence_transformers import SentenceTransformer
import chromadb
from chromadb.config import Settings

class UltraFastMarkdownRAG:
    """
    RAG para Markdown: 10-100x m√°s r√°pido que PDF
    """
    
    def __init__(self, model_size: str = "small"):
        """
        Inicializa con modelo peque√±o pero efectivo.
        
        Args:
            model_size: "tiny", "small", "base"
        """
        self.embedding_models = {
            "tiny": "sentence-transformers/all-MiniLM-L6-v2",  # 80MB
            "small": "sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2",  # 420MB
            "base": "intfloat/multilingual-e5-base"  # 1.1GB
        }
        
        print(f"üöÄ Cargando modelo {model_size}...")
        start = time.time()
        
        # Embeddings MUCHO m√°s r√°pidos (modelos peque√±os)
        self.embedder = SentenceTransformer(
            self.embedding_models[model_size],
            device='cpu'  # Puede correr hasta en CPU
        )
        
        # ChromaDB en memoria (ultra r√°pido)
        self.chroma_client = chromadb.Client(Settings(
            chroma_db_impl="duckdb+parquet",
            persist_directory=None,  # En memoria
            anonymized_telemetry=False
        ))
        
        self.collection = self.chroma_client.create_collection("markdown_docs")
        
        print(f"‚úÖ Sistema listo en {time.time()-start:.2f}s")
    
    def parse_markdown_semantic(self, md_content: str) -> List[Dict]:
        """
        Parseo sem√°ntico inteligente de Markdown.
        Preserva estructura: headers, listas, c√≥digo.
        """
        # Convertir a HTML para extracci√≥n estructurada
        html = markdown.markdown(md_content, extensions=['extra', 'codehilite'])
        soup = BeautifulSoup(html, 'html.parser')
        
        chunks = []
        current_chunk = ""
        current_header = ""
        
        # Extraer elementos sem√°nticos
        for element in soup.find_all(['h1', 'h2', 'h3', 'h4', 'p', 'li', 'code', 'pre']):
            tag_name = element.name
            text = element.get_text().strip()
            
            if not text:
                continue
            
            # Headers: crear nuevo chunk
            if tag_name.startswith('h'):
                if current_chunk:
                    chunks.append({
                        "content": current_chunk,
                        "header": current_header,
                        "type": "section"
                    })
                
                current_header = text
                current_chunk = f"# {text}\n\n"
            
            # P√°rrafos y listas: a√±adir al chunk actual
            elif tag_name in ['p', 'li']:
                if len(current_chunk) + len(text) < 1500:  # Chunk size √≥ptimo
                    current_chunk += text + "\n"
                else:
                    if current_chunk:
                        chunks.append({
                            "content": current_chunk,
                            "header": current_header,
                            "type": "section"
                        })
                    current_chunk = text + "\n"
            
            # C√≥digo: chunk separado
            elif tag_name in ['code', 'pre']:
                if current_chunk:
                    chunks.append({
                        "content": current_chunk,
                        "header": current_header,
                        "type": "section"
                    })
                chunks.append({
                    "content": f"```\n{text}\n```",
                    "header": "C√≥digo",
                    "type": "code"
                })
                current_chunk = ""
        
        # √öltimo chunk
        if current_chunk:
            chunks.append({
                "content": current_chunk,
                "header": current_header,
                "type": "section"
            })
        
        return chunks
    
    def index_markdown(self, md_path: str) -> None:
        """
        Indexa un archivo Markdown con chunking sem√°ntico.
        """
        print(f"üìÑ Indexando {md_path}...")
        start = time.time()
        
        # Leer contenido
        with open(md_path, 'r', encoding='utf-8') as f:
            content = f.read()
        
        # Parseo sem√°ntico
        chunks = self.parse_markdown_semantic(content)
        
        # Extraer textos para embeddings
        texts = [chunk["content"] for chunk in chunks]
        metadatas = [
            {
                "header": chunk["header"],
                "type": chunk["type"],
                "source": md_path,
                "length": len(chunk["content"])
            }
            for chunk in chunks
        ]
        
        # Embeddings en BATCH (ultra r√°pido)
        embeddings = self.embedder.encode(
            texts,
            batch_size=32,
            show_progress_bar=True,
            normalize_embeddings=True
        )
        
        # A√±adir a ChromaDB
        ids = [f"doc_{i}_{hash(text)%10000}" for i, text in enumerate(texts)]
        
        self.collection.add(
            embeddings=embeddings.tolist(),
            documents=texts,
            metadatas=metadatas,
            ids=ids
        )
        
        elapsed = time.time() - start
        print(f"‚úÖ Indexado: {len(chunks)} chunks en {elapsed:.2f}s")
        print(f"üìä Velocidad: {len(content)/elapsed/1000:.1f} KB/s")
    
    def query(self, question: str, k: int = 5) -> List[Dict]:
        """
        B√∫squeda sem√°ntica ultra r√°pida.
        """
        # Embedding de la pregunta
        query_embedding = self.embedder.encode([question]).tolist()[0]
        
        # Buscar
        results = self.collection.query(
            query_embeddings=[query_embedding],
            n_results=k,
            include=["documents", "metadatas", "distances"]
        )
        
        # Formatear con contexto estructural
        formatted = []
        for i, (doc, meta, dist) in enumerate(zip(
            results['documents'][0],
            results['metadatas'][0],
            results['distances'][0]
        ), 1):
            similarity = 1 - dist  # Convertir a score
            
            # A√±adir contexto del header
            header_info = f"Secci√≥n: {meta['header']}" if meta['header'] else ""
            
            formatted.append({
                "rank": i,
                "content": doc,
                "header": meta['header'],
                "type": meta['type'],
                "source": meta['source'],
                "score": round(similarity, 3),
                "context": f"{header_info}\n\n{doc[:500]}..."
            })
        
        return formatted
    
    def answer_with_context(self, question: str, context_chunks: List[str]) -> str:
        """
        Genera respuesta usando contextos de Markdown.
        Formatea el prompt para aprovechar la estructura MD.
        """
        context_text = "\n\n---\n\n".join(context_chunks)
        
        prompt = f"""Eres un asistente analizando documentos Markdown.

CONTEXTO DEL DOCUMENTO:
{context_text}

INSTRUCCIONES:
- El documento est√° en formato Markdown (# headers, **negrita**, etc.)
- Usa la estructura jer√°rquica para dar respuestas organizadas
- Si hay c√≥digo, c√≠talo con ``` bloques de c√≥digo
- Mant√©n el formato Markdown en tu respuesta cuando sea apropiado

PREGUNTA: {question}

RESPUESTA (en Markdown, estructurada):"""
        
        # Aqu√≠ integrar√≠as tu LLM local (Ollama, etc.)
        # Por ahora retornamos un placeholder
        return f"**Respuesta basada en {len(context_chunks)} secciones relevantes:**\n\nConsulta procesada: '{question}'"